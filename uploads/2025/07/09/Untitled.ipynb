{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f0d1ed-966d-494e-b2c6-07374e87d911",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'etl_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mschedule\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01metl_pipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_dataset1, preprocess_dataset2, preprocess_dataset3, preprocess_dataset4\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pipeline\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'etl_pipeline'"
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "from etl_pipeline import preprocess_dataset1, preprocess_dataset2, preprocess_dataset3, preprocess_dataset4\n",
    "import pandas as pd\n",
    "\n",
    "def run_pipeline():\n",
    "    print(\"\\nðŸ“… Starting scheduled ETL run...\")\n",
    "\n",
    "    # Load datasets\n",
    "    df1 = pd.read_csv(\"data/sample_data.csv\")\n",
    "    df2 = pd.read_json(\"data/sample_weather.json\")\n",
    "    df3 = pd.read_csv(\"data/titanic_data.csv\")\n",
    "    df4 = pd.read_csv(\"data/medical_data.csv\")\n",
    "\n",
    "    # Preprocess\n",
    "    df1_cleaned = preprocess_dataset1(df1)\n",
    "    df2_cleaned = preprocess_dataset2(df2)\n",
    "    df3_cleaned = preprocess_dataset3(df3)\n",
    "    df4_cleaned = preprocess_dataset4(df4)\n",
    "\n",
    "    # Save output\n",
    "    df1_cleaned.to_csv(\"output/preprocessed_sample_data.csv\", index=False)\n",
    "    df2_cleaned.to_csv(\"output/preprocessed_sample_weather.csv\", index=False)\n",
    "    df3_cleaned.to_csv(\"output/preprocessed_titanic.csv\", index=False)\n",
    "    df4_cleaned.to_csv(\"output/preprocessed_medical.csv\", index=False)\n",
    "\n",
    "    print(\"âœ… ETL completed and files updated.\")\n",
    "\n",
    "# Schedule the job daily at 02:00 AM\n",
    "schedule.every().day.at(\"02:00\").do(run_pipeline)\n",
    "\n",
    "print(\"â± Waiting for scheduled ETL run (daily at 02:00)...\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be682afe-3e2a-4720-8d75-4da9e62de16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting schedule\n",
      "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "138f839d-1cf0-42ed-a98b-cd0d3804f315",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2561361546.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    jupyter nbconvert --to script etl_pipeline.ipynb\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter nbconvert --to script etl_pipeline.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4198da8f-fb58-4724-bd98-d1d9828a259b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
